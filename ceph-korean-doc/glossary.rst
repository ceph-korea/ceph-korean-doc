===============
 Ceph Glossary
===============

Ceph is growing rapidly. As firms deploy Ceph, the technical terms such as
"RADOS", "RBD," "RGW" and so forth require corresponding marketing terms
that explain what each component does. The terms in this glossary are
intended to complement the existing technical terminology.

Sometimes more than one term applies to a definition. Generally, the first
term reflects a term consistent with Ceph's marketing, and secondary terms
reflect either technical terms or legacy ways of referring to Ceph systems.


.. glossary::

	Ceph 프로젝트
		Ceph 의 사람들, 소프트웨어, 미션 및 인프라에 대한 총칭

	cephx
		Ceph 인증 프로토콜. Cephx 는 Kerberos 와 비슷하게 동작하지만, 
		단일 실패 지점이 없습니다.

	Ceph
	Ceph 플랫폼
		`https://github.com/ceph`_ 에서 호스팅되는 모든 Ceph 소프트웨어

	Ceph 시스템
	Ceph 스택
		두개 이상의 Ceph 컴포넌트의 집합

	Ceph 노드
	노드
	호스트
		Ceph 시스템 내에 있는 단일 머신

	Ceph 스토리지 클러스터
	Ceph 오브젝트 스토어
	RADOS
	RADOS 클러스터
	안정적인 자율 분산 오브젝트 스토어
		유저의 데이터를 저장하는 스토리지 소프트웨어의 핵심 세트 (MON+OSD)

	Ceph 클러스터 맵
	클러스터 맵
		모니터 맵, OSD 맵, PG 맵, MDS 맵, CRUSH 맵으로 구성된 맵의 세트. `Cluster Map`_ 을 참고하세요.

	Ceph 오브젝트 스토리지
		Ceph 스토리지 클러스터와 Ceph 오브젝트 게이트웨이로 구성된 오브젝트 스토리지 "상품", 서비스 또는 기능

	Ceph 오브젝트 게이트웨이
	RADOS Gateway
	RGW
		Ceph 의 S3/Swift 게이트웨이 컴포넌트

	Ceph 블록 디바이스
	RBD
		Ceph 의 블록 스토리지 컴포넌트

	Ceph 블록 스토리지
		QEMU 또는 Xen 와 같은 하이퍼바이저, 및 ``libvirt`` 와 같은 하이퍼바이저 추상화
		계층과 함께 사용되는 블록 스토리지 "제품", 서비스 또는 기능

	Ceph 파일시스템
	CephFS
	Ceph FS
		The POSIX filesystem components of Ceph. Refer
		:ref:`CephFS Architecture <arch-cephfs>` and :ref:`ceph-filesystem` for
		more details.

	Cloud Platforms
	Cloud Stacks
		Third party cloud provisioning platforms such as OpenStack, CloudStack,
		OpenNebula, ProxMox, etc.

	Object Storage Device
	OSD
		A physical or logical storage unit (*e.g.*, LUN).
		Sometimes, Ceph users use the
		term "OSD" to refer to :term:`Ceph OSD Daemon`, though the
		proper term is "Ceph OSD".

	Ceph OSD Daemon
	Ceph OSD Daemons
	Ceph OSD
		The Ceph OSD software, which interacts with a logical
		disk (:term:`OSD`). Sometimes, Ceph users use the
		term "OSD" to refer to "Ceph OSD Daemon", though the
		proper term is "Ceph OSD".

	OSD id
		The integer that defines an OSD. It is generated by the monitors as part
		of the creation of a new OSD.

	OSD fsid
		This is a unique identifier used to further improve the uniqueness of an
		OSD and it is found in the OSD path in a file called ``osd_fsid``. This
		``fsid`` term is used interchangeably with ``uuid``

	OSD uuid
		Just like the OSD fsid, this is the OSD unique identifier and is used
		interchangeably with ``fsid``

	bluestore
		OSD BlueStore is a new back end for OSD daemons (kraken and newer
		versions). Unlike :term:`filestore` it stores objects directly on the
		Ceph block devices without any file system interface.

	filestore
		A back end for OSD daemons, where a Journal is needed and files are
		written to the filesystem.

	Ceph Monitor
	MON
		The Ceph monitor software.

	Ceph Manager
	MGR
		The Ceph manager software, which collects all the state from the whole
		cluster in one place.

	Ceph Manager Dashboard
	Ceph Dashboard
	Dashboard Module
	Dashboard Plugin
	Dashboard
		A built-in web-based Ceph management and monitoring application to
		administer various aspects and objects of the cluster. The dashboard is
		implemented as a Ceph Manager module. See :ref:`mgr-dashboard` for more
		details.

	Ceph Metadata Server
	MDS
		The Ceph metadata software.

	Ceph Clients
	Ceph Client
		The collection of Ceph components which can access a Ceph Storage
		Cluster. These include the Ceph Object Gateway, the Ceph Block Device,
		the Ceph Filesystem, and their corresponding libraries, kernel modules,
		and FUSEs.

	Ceph Kernel Modules
		The collection of kernel modules which can be used to interact with the
		Ceph System (e.g., ``ceph.ko``, ``rbd.ko``).

	Ceph Client Libraries
		The collection of libraries that can be used to interact with components
		of the Ceph System.

	Ceph Release
		Any distinct numbered version of Ceph.

	Ceph Point Release
		Any ad-hoc release that includes only bug or security fixes.

	Ceph Interim Release
		Versions of Ceph that have not yet been put through quality assurance
		testing, but may contain new features.

	Ceph Release Candidate
		A major version of Ceph that has undergone initial quality assurance
		testing and is ready for beta testers.

	Ceph Stable Release
		A major version of Ceph where all features from the preceding interim
		releases have been put through quality assurance testing successfully.

	Ceph Test Framework
	Teuthology
		The collection of software that performs scripted tests on Ceph.

	CRUSH
		Controlled Replication Under Scalable Hashing. It is the algorithm
		Ceph uses to compute object storage locations.

	CRUSH rule
		The CRUSH data placement rule that applies to a particular pool(s).

	Pool
	Pools
		Pools are logical partitions for storing objects.

	systemd oneshot
		A systemd ``type`` where a command is defined in ``ExecStart`` which will
		exit upon completion (it is not intended to daemonize)

	LVM tags
		Extensible metadata for LVM volumes and groups. It is used to store
		Ceph-specific information about devices and its relationship with
		OSDs.

.. _https://github.com/ceph: https://github.com/ceph
.. _Cluster Map: ../architecture#cluster-map
